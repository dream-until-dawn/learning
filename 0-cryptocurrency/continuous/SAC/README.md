# 状态价值函数:

# V(state) = 所有动作求和 -> 概率(action) \* Q(state,action)

# 对这个式子做变形得到:

# V(state) = 所有动作求和 -> 现概率(action) _ [旧概率(action) / 现概率(action)] _ Q(state,action)

# 初始时可以认为现概率和旧概率相等,但随着模型的更新,现概率会变化.

# 式子中的 Q(state,action)可以用蒙特卡洛法估计.

# 按照策略梯度的理论,状态价值取决于动作的质量,所以只要最大化 V 函数,就可以得到最好的动作策略.
